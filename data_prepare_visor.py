# this script prepares ActionVOS data from VISOR
# Action-aware labeling and action-guided weights are also generated by this script
# part of codes from VISOR-VOS repo
import numpy as np
import pandas as pd
import json
import os
import functools
import cv2
from tqdm import tqdm
from PIL import Image
import argparse

parser = argparse.ArgumentParser()
parser.add_argument('--VISOR_PATH', type=str, required=True)
args = parser.parse_args()
VISOR_PATH = args.VISOR_PATH

SAVE_PATH = 'dataset_visor'

# weight_only=True # only save weight
frame_mapping_json = os.path.join(VISOR_PATH,'frame_mapping.json')

EK_100_action_csv = {'train':'annotations/EPIC_100_train.csv',
                     'val':'annotations/EPIC_100_validation.csv',}
visor_hos_json = {'train':'annotations/visor_hos_train.json',
                     'val':'annotations/visor_hos_val.json',}

os.makedirs(SAVE_PATH,exist_ok=True)
os.makedirs(os.path.join(SAVE_PATH,'JPEGImages_Sparse'),exist_ok=True)
os.makedirs(os.path.join(SAVE_PATH,'JPEGImages_Sparse','train'),exist_ok=True)
os.makedirs(os.path.join(SAVE_PATH,'JPEGImages_Sparse','val'),exist_ok=True)
os.makedirs(os.path.join(SAVE_PATH,'ImageSets'),exist_ok=True)
os.makedirs(os.path.join(SAVE_PATH,'Annotations_Sparse'),exist_ok=True)
os.makedirs(os.path.join(SAVE_PATH,'Annotations_Sparse','train'),exist_ok=True)
os.makedirs(os.path.join(SAVE_PATH,'Annotations_Sparse','val'),exist_ok=True)
os.makedirs(os.path.join(SAVE_PATH,'Weights_Sparse'),exist_ok=True)
os.makedirs(os.path.join(SAVE_PATH,'Weights_Sparse','train'),exist_ok=True)
os.makedirs(os.path.join(SAVE_PATH,'Weights_Sparse','val'),exist_ok=True)

palette = Image.open('annotations/00000.png').getpalette()

def generate_action_with_mask(split='train'):
    '''
    find VISOR mask annotations for each action in EK-100
    return list of dicted RGB frames and annotation masks and weights
    [
        {
        'seq_id': int to identify sequence id,
        'video': str to identify video source,
        'verb''verb_class''noun''noun_class': verb noun annotation from EK-100,
        'narration': narration from EK-100,
        'start': start frame index from EK-100,
        'end': end frame index from EK-100,
        'object_classes': {1:{'name':xxx,'class_id':xxx,'positive':0/1,'handbox':0/1,'narration':0/1}, } for mask object labels in annotations,
        'sparse_frames': ['xxx.jpg','yyy.jpg'] for the SparseAnnotated frames in VISOR, but in EK-100 index
        }, ...
    ]
    for weights:
    *3 for negative obj mask.
    *2 for hand | narration obj mask. 
    *4 for hand & narration obj mask.
    *1 for other areas
    '''
    assert split in ['train','val']
    # read visor, EK-100, visor-hos
    frame_mapping_visor2ek = json.load(open(frame_mapping_json))
    EK_100_actions = pd.concat([pd.read_csv(EK_100_action_csv['train']),pd.read_csv(EK_100_action_csv['val'])])
    hos_annotations = map_hos(json.load(open(visor_hos_json[split])))
    
    list_of_dict = []
    seq_id = 0
    all_sparse_frames = 0
    for video in tqdm(frame_mapping_visor2ek.keys()):
        EK_100_actions_video = EK_100_actions[EK_100_actions['video_id']==video]
        if len(EK_100_actions_video) == 0:
            # not in EK-100 action train-val
            continue
        visor_annotations_json = os.path.join(VISOR_PATH,'GroundTruth-SparseAnnotations/annotations_corrected',split,video+'.json')
        if not os.path.exists(visor_annotations_json):
            # not in this split
            continue
        frame_mapping_ek2visor = reverse_mapping(frame_mapping_visor2ek[video])
        EK_100_actions_list = EK_100_actions_video.values.tolist()
        EK_100_actions_list = sorted(EK_100_actions_list, key=functools.cmp_to_key(cmp))
        frame_ek_np = np.array(sorted(list(frame_mapping_ek2visor.keys())))
        
        visor_annotations_sparse = json.load(open(visor_annotations_json))
        action_index = 0
        while action_index < len(EK_100_actions_list):
            action = EK_100_actions_list[action_index]
            dict = {}
            dict['seq_id'] = seq_id+1
            dict['video'] = video
            dict['start'] = action[6]
            dict['narration'] = action[8]
            dict['verb'] = action[9]
            dict['verb_class'] = action[10]
            dict['noun'] = action[11]
            dict['noun_class'] = action[12]

            dict['end'] = EK_100_actions_list[action_index][7]

            # check sparse annotation
            start_frame_ek = 'frame_{:010d}.jpg'.format(dict['start'])
            end_frame_ek = 'frame_{:010d}.jpg'.format(dict['end'])
            start = np.searchsorted(frame_ek_np,start_frame_ek)
            end = np.searchsorted(frame_ek_np,end_frame_ek)
            annotated_frames = []
            if start >= len(frame_ek_np):
                annotated_frames.append('no_annotation_for_this_action')
            elif start == end:
                if frame_ek_np[end] == end_frame_ek:
                    annotated_frames.append(end_frame_ek)
                else:
                    annotated_frames.append('no_annotation_for_this_action')
            else:
                for i in range(start,end):
                    annotated_frames.append(frame_ek_np[i])
                if end < len(frame_ek_np) and frame_ek_np[end] == end_frame_ek:
                    annotated_frames.append(end_frame_ek)
            dict['sparse_frames'] = annotated_frames
            if len(annotated_frames) <= 1:
                # filter actions with 0 or 1 sparse annotations
                action_index += 1
                continue           
            # generate sparse masks and pseudo labels
            res = generate_sparse_masks_pseudo_labels_weights(visor_annotations_sparse,frame_mapping_ek2visor,annotated_frames,hos_annotations=hos_annotations,all_nouns_cls=action[14])
            if res is None:
                # filter actions with no objects
                action_index += 1
                continue
            seq_id += 1 
            # prepare paths
            # EK_path_img = os.path.join(EK_PATH,video.split('_')[0],'rgb_frames',video)
            save_path_img_sparse = os.path.join(SAVE_PATH,'JPEGImages_Sparse',split,'{:08d}_{}_{}_{}'.format(seq_id,video,dict['verb'],dict['noun']))
            os.makedirs(save_path_img_sparse,exist_ok=True)
            save_path_png_sparse = save_path_img_sparse.replace('JPEGImages_Sparse','Annotations_Sparse')
            os.makedirs(save_path_png_sparse,exist_ok=True)
            save_path_weights_sparse = save_path_img_sparse.replace('JPEGImages_Sparse','Weights_Sparse')
            os.makedirs(save_path_weights_sparse,exist_ok=True)
            # save masks
            for i,m in enumerate(res[0]):
                m_pil = Image.fromarray(m,mode='P')
                m_pil = m_pil.resize((854,480),Image.NEAREST)
                m_pil.putpalette(palette)
                m_pil.save(os.path.join(save_path_png_sparse,annotated_frames[i].replace('jpg','png')))
                img = cv2.imread(os.path.join(VISOR_PATH,'GroundTruth-SparseAnnotations','rgb_frames',split,video,frame_mapping_ek2visor[annotated_frames[i]]))
                img = cv2.resize(img, (854, 480), interpolation=cv2.INTER_LINEAR)
                cv2.imwrite(os.path.join(save_path_img_sparse,annotated_frames[i]),img)
                all_sparse_frames += 1
            # save weights
            for i,m in enumerate(res[2]):
                m_pil = Image.fromarray(m,mode='P')
                m_pil = m_pil.resize((854,480),Image.NEAREST)
                m_pil.putpalette(palette)
                m_pil.save(os.path.join(save_path_weights_sparse,annotated_frames[i].replace('jpg','png')))
            dict['object_classes'] = res[1]

            list_of_dict.append(dict)
            action_index += 1
    json_object = json.dumps(list_of_dict) 
    with open(os.path.join(SAVE_PATH,'ImageSets',split+".json"), "w") as f:
        f.write(json_object)
    print('finished for {} set. {:d} actions, {:d} sparse frames'.format(split,seq_id,all_sparse_frames))    

def cmp(x,y):
    a_x,b_x,c_x = map(int,x[0][1:].split('_'))
    a_y,b_y,c_y = map(int,y[0][1:].split('_'))
    if (a_x,b_x,c_x)<(a_y,b_y,c_y):
        return -1
    return 1

def reverse_mapping(dict):
    # change the key:value of the dict to value:key
    res = {}
    for k in dict.keys():
        res[dict[k]]=k
    return res

def map_hos(hos_annotations):
    # get hos annotations to {'file_name':{'bboxs':[bbox1,bbox2],'masks':[segment1,segment2]}}
    res = {}
    id2file = {}
    for img in hos_annotations['images']:
        id2file[img['id']] = img['file_name']
    for ann in hos_annotations['annotations']:
        if ann['category_id'] == 2:
            file_name = id2file[ann['image_id']]
            if file_name not in res:
                res[file_name] = {'bboxs':[],'masks':[]}
            res[file_name]['bboxs'].append(ann['bbox'])
            res[file_name]['masks'].append(ann['segmentation'])
    return res

def generate_sparse_masks_pseudo_labels_weights(visor_annotations,frame_mapping_ek2visor,annotated_frames,hos_annotations=None,all_nouns_cls=[]):
    '''
    generate VISOR masks, pseudo cls labels, and loss weights for the action sequence
    read sparse annotated mask from VISOR
    and save the obj-color mapping

    annotated_frames = [framexxxxxxxxxx.jpg,...,]

    return ([mask_np],obj_map,[weight_np])
    '''
    # get all annotated objects
    all_obj_map = get_all_obj_map(visor_annotations,frame_mapping_ek2visor,annotated_frames)
    if all_obj_map is None:
        return None
    # the following part is action-aware labeling
    # get objects in hand boxs
    hand_obj = get_hand_obj(visor_annotations,frame_mapping_ek2visor,annotated_frames,hos_annotations)
    # get objects in narrations
    narration_obj = get_narration_obj(all_obj_map,all_nouns_cls)
    # label postive objects
    for obj in all_obj_map.keys():
        # deal with hand
        if hand_obj is None:
            all_obj_map[obj].append(0)
        else:
            if obj in hand_obj:
                all_obj_map[obj].append(1)
            else:
                all_obj_map[obj].append(0)
        # deal with narration
        if narration_obj is None:
            all_obj_map[obj].append(0)
        else:
            if obj in narration_obj:
                all_obj_map[obj].append(1)
            else:
                all_obj_map[obj].append(0)
        # deal with pos
        if all_obj_map[obj][-1] or all_obj_map[obj][-2]:
            all_obj_map[obj].append(1)
        else:
            all_obj_map[obj].append(0)
    # the following part is mask and action-guided weights
    masks = []
    weights = []
    for frame in annotated_frames:
        frame_visor = frame_mapping_ek2visor[frame]
        # draw pseudo mask
        for ann in visor_annotations['video_annotations']:
            if ann['image']['name'] == frame_visor:
                mask_np = np.zeros([1080,1920],dtype=np.uint8)
                for entity in ann['annotations']:
                    polygons = []
                    for segment in entity['segments']:
                        polygons.append(segment)
                    ps = []
                    #store the polygons in one list. One object may has more than 1 polygon
                    for poly in polygons:
                        if poly == []:
                            poly = [[0.0, 0.0]]
                        ps.append(np.array(poly, dtype=np.int32))
                    if (entity['name'] in all_obj_map.keys()):
                        color = all_obj_map[entity['name']][0]
                        cv2.fillPoly(mask_np, ps, (color, color, color))
                masks.append(mask_np)
                break
        # draw action-guided weight
        for ann in visor_annotations['video_annotations']:
            if ann['image']['name'] == frame_visor:
                weight_np = np.zeros([1080,1920],dtype=np.uint8)
                if frame_visor in hos_annotations:
                    # draw hand mask
                    hand_mask_np = np.zeros([1080,1920],dtype=np.uint8)
                    hand_bbox_np = np.zeros([1080,1920],dtype=np.uint8)
                    for hom in hos_annotations[frame_visor]['masks']:
                        ps = []
                        for seg in hom:
                            seg = np.array(seg, dtype=np.int32)
                            poly = np.reshape(seg,(-1,2))
                            ps.append(poly)
                        cv2.fillPoly(hand_mask_np, ps, (2, 2, 2))
                    # draw hand bboxs
                    bboxs = hos_annotations[frame_visor]['bboxs']#xywh
                    for bbox in bboxs:
                        #bbox_xyxy = [bbox[0], bbox[1], bbox[0]+bbox[2], bbox[1]+bbox[3]]
                        hand_bbox_np[bbox[1]:bbox[1]+bbox[3],bbox[0]:bbox[0]+bbox[2]] = 1
                for entity in ann['annotations']:
                    polygons = []
                    for segment in entity['segments']:
                        polygons.append(segment)
                    ps = []
                    #store the polygons in one list. One object may has more than 1 polygon
                    for poly in polygons:
                        if poly == []:
                            poly = [[0.0, 0.0]]
                        ps.append(np.array(poly, dtype=np.int32))
                    # for narration objs
                    if ((narration_obj is not None) and (entity['name'] in narration_obj)):
                        cv2.fillPoly(weight_np, ps, (2, 2, 2))
                    else:
                    # for negative objs
                        if entity['name'] in all_obj_map.keys():
                            if frame_visor in hos_annotations:
                                if not segment_in_box(ps,hos_annotations[frame_visor]['bboxs']):
                                    cv2.fillPoly(weight_np, ps, (3, 3, 3))
                            else:
                                if all_obj_map[entity['name']][-1] == 0:
                                    cv2.fillPoly(weight_np, ps, (3, 3, 3))
                # for hand objs
                if frame_visor in hos_annotations:
                    weight_np = np.where(hand_bbox_np==1, weight_np*2, weight_np)
                    weight_np = np.where(weight_np == 0, hand_mask_np, weight_np)   
                weight_np = np.where(weight_np == 0, 1, weight_np)
                weights.append(weight_np)
                break
    # change all_obj_map to {color:{'name': ,'class_id':, 'positive':, 'handbox':, 'narration':}}
    obj_map_res = {}
    for name in all_obj_map.keys():
        color = all_obj_map[name][0]
        obj_map_res[color] = {'name':name,'class_id':all_obj_map[name][1],'handbox':all_obj_map[name][2], 'narration':all_obj_map[name][3],'positive':all_obj_map[name][4]} 
    return (masks,obj_map_res,weights)

def segment_in_box(ps,bboxs):
    '''
    check if a segmentation mask (in ps) is inside any of the bboxs.
    '''
    segment = np.zeros([1080,1920],dtype=np.uint8)
    cv2.fillPoly(segment, ps, (1, 1, 1))
    box = np.zeros([1080,1920],dtype=np.uint8)
    for bbox in bboxs:
        box[bbox[1]:bbox[1]+bbox[3],bbox[0]:bbox[0]+bbox[2]] = 1
    return (segment*box).any()

def get_all_obj_map(visor_annotations,frame_mapping_ek2visor,annotated_frames):
    '''
    get obj_map: {name:[color,class_id]}. would be added to [color,class_id,handbox,narration,pos]
    should be sorted here. set() would random hash every run
    '''
    objects = set()
    cls_id = {}
    for frame in annotated_frames:
        frame_visor = frame_mapping_ek2visor[frame]
        for ann in visor_annotations['video_annotations']:
            if ann['image']['name'] == frame_visor:
                for entity in ann['annotations']: #loop over each object
                    if len(entity["segments"]) > 0: #if there is annotation for this object, add it
                        objects.add(entity["name"])
                        cls_id[entity["name"]] = entity["class_id"]
                break
    if len(objects) == 0:
        return None
    objects = list(objects)
    objects = sorted(objects)# must be sorted!
    obj_map = {}
    for i,obj in enumerate(objects):
        obj_map[obj] = [i+1,cls_id[obj]]
    return obj_map   

def get_hand_obj(visor_annotations,frame_mapping_ek2visor,annotated_frames,hos_annotations):
    '''
    get obj: set of positive objs (in class name)
    '''
    objects = set()
    for frame in annotated_frames:
        frame_visor = frame_mapping_ek2visor[frame]
        for ann in visor_annotations['video_annotations']:
            if ann['image']['name'] == frame_visor:
                # draw all annotation
                mask_np = np.zeros([1080,1920],dtype=np.uint8)# yx
                tmp_map = {}
                for i,entity in enumerate(ann['annotations']):
                    polygons = []
                    for segment in entity['segments']:
                        polygons.append(segment)
                    ps = []
                    #store the polygons in one list. One object may has more than 1 polygon
                    for poly in polygons:
                        if poly == []:
                            poly = [[0.0, 0.0]]
                        ps.append(np.array(poly, dtype=np.int32))  
                    cv2.fillPoly(mask_np, ps, (i+1, i+1, i+1))
                    tmp_map[i+1] = entity['name']
                    if entity['name'] in ['left hand','right hand']:
                        objects.add(entity['name'])
                # get bboxs and bbox objs
                if frame_visor in hos_annotations:
                    bboxs = hos_annotations[frame_visor]['bboxs']#xywh
                    for bbox in bboxs:
                        #bbox_xyxy = [bbox[0], bbox[1], bbox[0]+bbox[2], bbox[1]+bbox[3]]
                        crop = mask_np[bbox[1]:bbox[1]+bbox[3],bbox[0]:bbox[0]+bbox[2]]
                        unique = np.unique(crop)
                        for u in unique:
                            if u != 0:
                                objects.add(tmp_map[u])
                break
    if len(objects) == 0:
        return None
    return objects

def get_narration_obj(all_obj_map,all_nouns_cls):
    # EK-100 annotation all_nouns_cls
    objects = set()
    all_nouns_cls = eval(all_nouns_cls)
    for obj in all_obj_map.keys():
        if all_obj_map[obj][1] in all_nouns_cls:
            objects.add(obj)
    if len(objects) == 0:
        return None
    return objects

def main():
    generate_action_with_mask('train')
    generate_action_with_mask('val')

if __name__ == '__main__':
    main()