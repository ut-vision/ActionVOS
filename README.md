ActionVOS: Actions as Prompts for Video Object Segmentation
<!-- ---
[![arXiv](https://img.shields.io/badge/arXiv-2403.04381-DodgerBlue.svg?style=plastic)](https://arxiv.org/pdf/2403.04381.pdf) -->

Our paper is accepted by **ECCV-2024**

<div align=center>  <img src="figures/ActionVOS.png" alt="ActionVOS" width="500" align="bottom" /> </div>

**Picture:**  *Overview of the proposed ActionVOS setting*

<div align=center>  <img src="./figures/method.png" alt="method" width="800" align="center" /> </div>

**Picture:**  *The proposed method in our paper.*

---

This repository contains the official PyTorch implementation of the following paper:

> **ActionVOS: Actions as Prompts for Video Object Segmentation**<br>
Liangyang Ouyang, Ruicong Liu, Yifei Huang, Ryosuke Furuta, and Yoichi Sato<br> <!-- >  https://arxiv.org/abs/  -->
> 
>**Abstract:**  

## Resources

Material related to our paper is available via the following links:

- Paper: 
- Code: https://github.com/ut-vision/ActionVOS
- VISOR dataset: https://epic-kitchens.github.io/VISOR/
- VOST dataset: https://www.vostdataset.org/data.html
- VSCOS dataset: https://github.com/venom12138/VSCOS
- ReferFormer Model: https://github.com/wjn922/ReferFormer

## Requirements

* Our experiment is tested with Python 3.8, PyTorch 1.11.0. 

## Playing with ActionVOS

### Data preparation

### Training

### Inference

## Citation

If this work or code is helpful in your research, please cite:

<!-- ```latex
@inproceedings{liu2024single,
 title = {Single-to-Dual-View Adaptation for Egocentric 3D Hand Pose Estimation},
 author = {Liu, Ruicong and Ohkawa, Takehiko and Zhang, Mingfang and Sato, Yoichi},
 booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
 pages = {0--0},
 year = {2024}
}
``` -->

## Contact

For any questions, including algorithms and datasets, feel free to contact me by email: `oyly(at)iis.u-tokyo.ac.jp`